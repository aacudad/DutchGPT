{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding name\tOpenAI models\n",
    "# o200k_base\tgpt-4o, gpt-4o-mini\n",
    "# cl100k_base\tgpt-4-turbo, gpt-4, gpt-3.5-turbo, text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large\n",
    "# p50k_base\tCodex models, text-davinci-002, text-davinci-003\n",
    "# r50k_base (or gpt2)\tGPT-3 models like davinci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data in the notebook: ./archived_projects/llm_from_scratch/notebooks/train_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_format= [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tiktoken  # Library for tokenization (e.g., OpenAI token encodings)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Tokenization\n",
    "# ---------------------------\n",
    "def tokenize(text_input: str, encoding_type: str = \"r50k_base\") -> tuple[list, int]:\n",
    "    \"\"\"\n",
    "    Converts input text into token using the specified encoding.\n",
    "    \n",
    "    Returns:\n",
    "        tokens: List of token.\n",
    "        vocab_size: The size of the vocabulary for the chosen encoding.\n",
    "    \"\"\"\n",
    "    enc = tiktoken.get_encoding(encoding_type)\n",
    "    vocab_size = enc.n_vocab\n",
    "    tokens = enc.encode(text_input)\n",
    "    return tokens, vocab_size\n",
    "\n",
    "# ---------------------------\n",
    "# 2. GPT Model Definition (Decoder-Only Transformer)\n",
    "# ---------------------------\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_heads, n_layers, max_seq_length):\n",
    "        \"\"\"\n",
    "        Initializes a GPT language model using transformer encoder layers as a decoder.\n",
    "        \n",
    "        Key Components:\n",
    "          - Token Embedding: Maps token indices to dense vectors.\n",
    "          - Positional Embedding: Adds position information to token embeddings.\n",
    "          - Transformer Encoder Layers: Each layer internally performs:\n",
    "                * Multi-head self-attention with causal masking.\n",
    "                * A feedforward (point-wise) network.\n",
    "          - Final LayerNorm and Linear Head: Normalizes the output and projects it to logits over the vocabulary.\n",
    "        \n",
    "        Parameters:\n",
    "            vocab_size (int): Number of tokens in the vocabulary.\n",
    "            embedding_dim (int): Dimensionality of embeddings.\n",
    "            n_heads (int): Number of attention heads in the multi-head self-attention.\n",
    "            n_layers (int): Number of transformer encoder layers.\n",
    "            max_seq_length (int): Maximum sequence length for positional embeddings.\n",
    "        \"\"\"\n",
    "        super(GPTModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_length, embedding_dim)\n",
    "        # Create a list of transformer encoder layers.\n",
    "        # Each nn.TransformerEncoderLayer includes:\n",
    "        #   1. Multi-head self-attention (the \"self-attention\" mechanism).\n",
    "        #   2. A feedforward network (point-wise, applied to each token independently).\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=n_heads, batch_first=True)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        # Final layer normalization before projecting to output vocabulary logits.\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        # Linear layer mapping hidden state to vocabulary logits.\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Steps:\n",
    "          1. Create token and positional embeddings.\n",
    "          2. Add embeddings together.\n",
    "          3. Create a causal mask (lower triangular) to ensure a token only attends to previous tokens.\n",
    "          4. For each transformer encoder layer:\n",
    "              - Internally performs multi-head self-attention (using the causal mask).\n",
    "              - Applies a feedforward network.\n",
    "          5. Normalize the final outputs and project to vocabulary logits.\n",
    "        \n",
    "        Parameters:\n",
    "            x (Tensor): Input tensor with shape (batch_size, seq_length).\n",
    "            \n",
    "        Returns:\n",
    "            logits (Tensor): Unnormalized log probabilities with shape (batch_size, seq_length, vocab_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.size()\n",
    "        # Generate positional indices for each token in the sequence.\n",
    "        positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0).expand(batch_size, seq_length)\n",
    "        token_emb = self.token_embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        pos_emb = self.position_embedding(positions)  # (batch_size, seq_length, embedding_dim)\n",
    "        x = token_emb + pos_emb  # Combine token and positional embeddings\n",
    "        \n",
    "        # Create a causal (lower triangular) mask:\n",
    "        # This mask prevents a token from attending to any future tokens.\n",
    "        mask = torch.tril(torch.ones(seq_length, seq_length, device=x.device)).bool()\n",
    "        \n",
    "        # Pass through each transformer encoder layer:\n",
    "        # Each layer internally executes:\n",
    "        #   - Multi-head self-attention using the provided causal mask (self-attention happens here).\n",
    "        #   - A feedforward (point-wise) layer.\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=mask)\n",
    "        \n",
    "        # Apply final layer normalization.\n",
    "        x = self.ln_f(x)\n",
    "        # Project the normalized hidden states to logits over the vocabulary.\n",
    "        logits = self.head(x)  # (batch_size, seq_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, start_tokens, max_new_tokens=20, temperature=1.5, top_k=50, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Autoregressively generate text using the trained model.\n",
    "        \n",
    "        Generation Steps:\n",
    "          - Start with an initial prompt (start_tokens).\n",
    "          - Iteratively predict the next token using the model.\n",
    "          - Apply temperature scaling to logits.\n",
    "          - Use top-k and nucleus (top-p) sampling to limit the token selection.\n",
    "        \n",
    "        Parameters:\n",
    "            start_tokens (Tensor): Initial tokens with shape (batch_size, seq_length).\n",
    "            max_new_tokens (int): Number of tokens to generate.\n",
    "            temperature (float): Controls randomness (higher => more random).\n",
    "            top_k (int): Keeps only top k tokens for sampling.\n",
    "            top_p (float): Cumulative probability threshold for nucleus sampling.\n",
    "        \n",
    "        Returns:\n",
    "            generated (Tensor): Tensor with generated token IDs appended to the prompt.\n",
    "        \"\"\"\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        generated = start_tokens.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass: compute logits for current sequence\n",
    "            logits = self(generated)  # (batch_size, current_seq_length, vocab_size)\n",
    "            next_logits = logits[:, -1, :] / temperature  # Focus on the last token's logits\n",
    "\n",
    "            # --------- Multi-head Self-Attention and Feedforward Layers Occur in Each Transformer Layer ---------\n",
    "            # Note: These operations happen within each encoder layer during the forward() call above.\n",
    "            # ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "            # Top-k filtering: restrict sampling to the top k tokens\n",
    "            if top_k is not None and top_k > 0:\n",
    "                topk_values, topk_indices = torch.topk(next_logits, k=top_k, dim=-1)\n",
    "                threshold = topk_values[:, -1].unsqueeze(-1)\n",
    "                next_logits = torch.where(next_logits < threshold, torch.full_like(next_logits, float('-inf')), next_logits)\n",
    "\n",
    "            # Nucleus (top-p) filtering: restrict sampling to tokens within the cumulative probability top_p.\n",
    "            if top_p is not None and top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True, dim=-1)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_logits[cumulative_probs > top_p] = float('-inf')\n",
    "                next_logits = torch.zeros_like(next_logits).scatter_(dim=-1, index=sorted_indices, src=sorted_logits)\n",
    "\n",
    "            # Clean logits (handle potential NaNs/Infs) and sample the next token.\n",
    "            next_logits = torch.nan_to_num(next_logits, nan=-1e10, posinf=1e10, neginf=-1e10)\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "        return generated\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Prepare an Expanded Text Dataset\n",
    "# ---------------------------\n",
    "texts = [\"Here comes the text data\"]\n",
    "max_length = 1024  # Maximum sequence length for each example\n",
    "tokenized_texts = []\n",
    "vocab_size = None\n",
    "for text in texts:\n",
    "    tokens, vs = tokenize(text)\n",
    "    tokens = tokens[:max_length]  # Truncate sequences longer than max_length\n",
    "    tokenized_texts.append(torch.tensor(tokens, dtype=torch.long))\n",
    "    if vocab_size is None:\n",
    "        vocab_size = vs\n",
    "\n",
    "# Pad sequences to the same length for batch processing.\n",
    "padded_sequences = rnn_utils.pad_sequence(tokenized_texts, batch_first=True, padding_value=0)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Create a Dataset with Shifted Targets\n",
    "# ---------------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        \"\"\"\n",
    "        Dataset for language modeling.\n",
    "        \n",
    "        Each sample consists of:\n",
    "            - Input: A sequence of token IDs.\n",
    "            - Target: The same sequence shifted by one token.\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sequences.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        if seq.size(0) < 2:\n",
    "            raise ValueError(\"Sequence length must be at least 2 for shifting\")\n",
    "        # Input tokens (all tokens except the last)\n",
    "        input_ids = seq[:-1]\n",
    "        # Target tokens (all tokens except the first)\n",
    "        target_ids = seq[1:]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "dataset = TextDataset(padded_sequences)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Set up the Device and Instantiate the Model\n",
    "# ---------------------------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "embedding_dim = 512   # Embedding dimension for tokens and positions\n",
    "n_heads = 8           # Number of attention heads in each multi-head self-attention block\n",
    "n_layers = 6          # Number of transformer encoder layers (each with self-attention and feedforward)\n",
    "max_seq_length = padded_sequences.size(1)  # Sequence length derived from padded data\n",
    "\n",
    "# Instantiate the GPT transformer model\n",
    "model = GPTModel(vocab_size=vocab_size,\n",
    "                 embedding_dim=embedding_dim,\n",
    "                 n_heads=n_heads,\n",
    "                 n_layers=n_layers,\n",
    "                 max_seq_length=max_seq_length).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Loss function for next-token prediction\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Optimizer\n",
    "\n",
    "# Optionally disable torch.compile if it causes issues:\n",
    "# if hasattr(torch, \"compile\"):\n",
    "#     model = torch.compile(model)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Training Loop Over Epochs\n",
    "# ---------------------------\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "    for input_ids, target_ids in data_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        logits = model(input_ids)\n",
    "        # Reshape logits and targets for loss computation:\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        target_ids = target_ids.view(-1)\n",
    "        loss = criterion(logits, target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for input_ids, target_ids in data_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        target_ids = target_ids.view(-1)\n",
    "        loss = criterion(logits, target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Inference\n",
    "# ---------------------------\n",
    "def generate_text(model, prompt, max_new_tokens=30, temperature=1.5, top_k=50, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generates text from a given prompt using the trained model.\n",
    "    \n",
    "    Steps:\n",
    "      - Tokenize the input prompt.\n",
    "      - Autoregressively generate new tokens.\n",
    "      - Decode the generated tokens back to text.\n",
    "    \n",
    "    Parameters:\n",
    "        model (nn.Module): The transformer-based language model.\n",
    "        prompt (str): The initial text prompt.\n",
    "        max_new_tokens (int): Number of new tokens to generate.\n",
    "        temperature (float): Controls randomness in sampling.\n",
    "        top_k (int): Parameter for top-k filtering.\n",
    "        top_p (float): Parameter for nucleus (top-p) sampling.\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    tokens, _ = tokenize(prompt)\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "    generated_ids = model.generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                                   temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "    # Decode token ids back to text using a specific encoding\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    return enc.decode(generated_ids[0].tolist())\n",
    "\n",
    "prompt_text = \"College\"\n",
    "generated = generate_text(model, prompt_text, max_new_tokens=30, temperature=1.5, top_k=50, top_p=0.9)\n",
    "print(\"Generated text:\\n\", generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (FINETUNE)",
   "language": "python",
   "name": "finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
